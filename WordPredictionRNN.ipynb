{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction\n",
    "## Build and train a model to perform word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "Our dataset consists of headlines from the New York Times newspaper over the course of several months. We'll start by reading in all the headlines from the articles. The articles are in CSV files, so we can use pandas to read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_dir = 'data/nyt_dataset/articles/'\n",
    "\n",
    "all_headlines = []\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Read in all the data from the CSV file\n",
    "        headlines_df = pd.read_csv(nyt_dir + filename)\n",
    "        # Add all of the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My Beijing: The Sacred City',\n",
       " '6 Million Riders a Day, 1930s Technology',\n",
       " 'Seeking a Cross-Border Conference',\n",
       " 'Questions for: ‘Despite the “Yuck Factor,” Leeches Are Big in Russian Medicine’',\n",
       " 'Who Is a ‘Criminal’?',\n",
       " 'An Antidote to Europe’s Populism',\n",
       " 'The Cost of a Speech',\n",
       " 'Degradation of the Language',\n",
       " 'On the Power of Being Awful',\n",
       " 'Trump Garbles Pitch on a Revised Health Bill',\n",
       " 'What’s Going On in This Picture? | May 1, 2017',\n",
       " 'Unknown',\n",
       " 'When Patients Hit a Medical Wall',\n",
       " 'Unknown',\n",
       " 'For Pregnant Women, Getting Serious About Whooping Cough',\n",
       " 'Unknown',\n",
       " 'New York City Transit Reporter in Wonderland: Riding the London Tube',\n",
       " 'How to Cut an Avocado Without Cutting Yourself',\n",
       " 'In Fictional Suicide, Health Experts Say They See a Real Cause for Alarm',\n",
       " 'Claims of Liberal Media Bias Hit ESPN, Too']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "1. Remove all headlines with the value of \"Unknown\"\n",
    "2. Remove punctuation and set all sentences to lower case. (easier to train. For our purposes, there is little or no difference between a line ending with \"!\" or \"?\" or whether words are capitalized.)\n",
    "3. Tokenization: \n",
    "    \n",
    "    a. Separate a piece of text into smaller chunks (tokens), which in this case are words.\n",
    "    \n",
    "    b.take each of the words that appears in our dataset and represent it with a number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "# Remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)\n",
    "\n",
    "# Tokenize the words in our headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 139, 'panama': 2732, 'canal': 7047}\n"
     ]
    }
   ],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "               if key in ['a','man','a','plan','a','canal','panama']}\n",
    "print(subset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [139], [2], [82], [2], [7047], [2732]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how the tokenizer saves the words:\n",
    "tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences\n",
    "Now that we've tokenized the data, we will create sequences of tokens from the headlines. \n",
    "\n",
    "These sequences are what we will train our deep learning model on.\n",
    "\n",
    "\"nvidia launches ray tracing gpus\"\n",
    "\n",
    "nvidia - 5, launches - 22, ray - 94, tracing - 16, gpus - 102. \n",
    "\n",
    "The full sequence would be: [5, 22, 94, 16, 102]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my beijing', 'my beijing the', 'my beijing the sacred', 'my beijing the sacred city', '6 million']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[52, 1616],\n",
       " [52, 1616, 1],\n",
       " [52, 1616, 1, 1992],\n",
       " [52, 1616, 1, 1992, 125],\n",
       " [126, 346]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to sequence of tokens \n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Sequences\n",
    "Right now our sequences are of various lengths.\n",
    "\n",
    "For our model to be able to train on the data, we need to make all the sequences the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   52, 1616], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Predictors and Target\n",
    "Split up the sequences into predictors and a target.\n",
    "\n",
    "The last word of the sequence will be our target, and the first words of the sequence will be our predictors.\n",
    "\n",
    "As an example, take a look at the full headline: \"nvidia releases ampere graphics cards\"\n",
    "<table>\n",
    "<tr><td>PREDICTORS </td> <td>           TARGET </td></tr>\n",
    "<tr><td>nvidia                   </td> <td>  releases </td></tr>\n",
    "<tr><td>nvidia releases               </td> <td>  ampere </td></tr>\n",
    "<tr><td>nvidia releases ampere      </td> <td>  graphics</td></tr>\n",
    "<tr><td>nvidia releases ampere graphics </td> <td>  cards</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1616,    1, 1992,  125,  346], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors are every word except the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# Labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like our earlier sections, these targets are categorical.\n",
    "# We are predicting one word out of our possible total vocabulary. \n",
    "# Instead of the network predicting scalar numbers, we will have it predict binary categories.\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "* Using new layers to deal with our sequential data.\n",
    "1. Embedding Layer - take the tokenized sequences and will learn an embedding for all of the words in the training dataset. Mathematically, embeddings work the same way as a neuron in a neural network, but conceptually, their goal is to reduce the number of dimensions for some or all of the features. In this case, it will represent each word as a vector, and the information within that vector will contain the relationships between each word.\n",
    "2. LSTM - Very important layer, is a long short term memory layer (LSTM). An LSTM is a type of recurrent neural network or RNN. Unlike traditional feed-forward networks that we've seen so far, recurrent networks have loops in them, allowing information to persist. New information (x) gets passed in to the network, which spits out a prediction (h). Additionally, information from that layer gets saved, and used as input for the next prediction. This may seem a bit complicated, but let's look at it unrolled.We can see that when a new piece of data (x) is fed into the network, that network both spits out a prediction (h) and also passes some information along to the next layer. That next layer gets another piece of data, but gets to learn from the layer before it as well. Traditional RNNs suffer from the issue of more recent information contributing more than information from further back. LSTMs are a special type of recurrent layer that are able to learn and retain longer term information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is max sequence length - 1, as we've removed the last word for the label\n",
    "input_len = max_sequence_len - 1 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer\n",
    "model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 27, 10)            117530    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11753)             1187053   \n",
      "=================================================================\n",
      "Total params: 1,348,983\n",
      "Trainable params: 1,348,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Fit the model.\n",
    "\n",
    "30 epochs will take a few minutes.\n",
    "\n",
    "we don't have a training or validation accuracy score because its a problem of text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 7.8900\n",
      "Epoch 2/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 7.4806\n",
      "Epoch 3/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 7.2907\n",
      "Epoch 4/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 7.0829\n",
      "Epoch 5/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 6.8640\n",
      "Epoch 6/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 6.6284\n",
      "Epoch 7/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 6.3868\n",
      "Epoch 8/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 6.1443\n",
      "Epoch 9/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 5.9010\n",
      "Epoch 10/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 5.6667\n",
      "Epoch 11/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 5.4467\n",
      "Epoch 12/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 5.2344\n",
      "Epoch 13/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 5.0381\n",
      "Epoch 14/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.8517\n",
      "Epoch 15/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.6740\n",
      "Epoch 16/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.5076\n",
      "Epoch 17/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.3500\n",
      "Epoch 18/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.2032\n",
      "Epoch 19/30\n",
      "1666/1666 [==============================] - 8s 5ms/step - loss: 4.0681\n",
      "Epoch 20/30\n",
      "1358/1666 [=======================>......] - ETA: 1s - loss: 3.8985"
     ]
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "1. Start with a seed text\n",
    "2. Prepare it in the same way we prepared our dataset (tokenizing and padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_list, verbose=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_next_token(\"today in new york\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our tokenizer to decode the predicted word:\n",
    "tokenizer.sequences_to_texts([prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Headlines\n",
    "Predict headlines of more than just one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new headline of arbitrary length.\n",
    "def generate_headline(seed_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        # Predict next token\n",
    "        prediction = predict_next_token(seed_text)\n",
    "        # Convert token to word\n",
    "        next_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "        # Add next word to the headline. This headline will be used in the next pass of the loop.\n",
    "        seed_text += \" \" + next_word\n",
    "    # Return headline as title-case\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some headlines!\n",
    "seed_texts = [\n",
    "    'washington dc is',\n",
    "    'today in new york',\n",
    "    'the school district has',\n",
    "    'crime has become']\n",
    "for seed in seed_texts:\n",
    "    print(generate_headline(seed, next_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "1. Most of the headlines make some kind of grammatical sense, but don't necessarily indicate a good contextual understanding.\n",
    "2. Try to run on more epochs\n",
    "3. Other improvements: using pretrained embeddings with Word2Vec or GloVe, rather than learning them during training as we did with the Keras Embedding layer.\n",
    "4. NLP has moved beyond simple LSTM models to Transformer-based pre-trained models, which are able to learn language context from huge amounts of textual data such as Wikipedia. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
